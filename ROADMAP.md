# ğŸ§¬ Protein/Genomics LM Roadmap

---

## Stage 1 â€“ Toy Scale (MacBookâ€‘friendly)

*Goal: Learn mechanics â€” tokenization, training, **and** a repeatable 6â€‘step interpretability pipeline.*

**Minimal model**: 2 layers Â· 4 heads Â· d=128 Â· 5 epochs (YAMLâ€‘driven).
**Core outputs per run**: `runs/<run_id>/config.yaml`, `weights.pt`, `artifacts.npz`, `itos.txt`, optional `motif_clusters.npz`, `one_cds__best.tsv`.

### Projects

1. **Codon Language Model**

* Train on a handful of bacterial genomes (\~200kâ€“1M params).
* Evaluate base LM metrics (loss, perplexity).
* Produce artifacts with `collect_artifacts_yaml.py`.

2. **Mutation Maps (geneâ€‘level probing)**

* For a gene (e.g., *lacZ*), compute Î” logâ€‘likelihood for singleâ€‘codon substitutions.
* Compare "hot" positions vs conservation and known motifs.

3. **Motif Mining**

* Extract hiddenâ€‘state embeddings (from artifacts) and cluster subsequences.
* Validate candidate motifs against known protein family motifs or positional conservation.

---

## The 6â€‘Step Interpretability Pipeline (summary)

Highâ€‘level goals per step (full details in MANUAL.md):

- Step 1 â€” Frequencies: codon usage, firstâ€‘position patterns, stops/starts as punctuation.
- Step 2 â€” Embeddings: PCA/NN semantics; synonymous clusters; quality scores.
- Step 3 â€” Attention: heads specializing to starts/stops, frames, motif boundaries.
- Step 4 â€” Nextâ€‘token probes: conditional distributions after biological prefixes.
- Step 5 â€” Saliency: which tokens/regions drive predictions; motif spikes.
- Step 6 â€” Linear probes: decode AA identity/properties from embeddings.

All steps read `runs/<run_id>/artifacts.npz` (plus optional labels) and write charts/tables under `runs/<run_id>/`.

---

## Stage 2 â€“ Midâ€‘Scale (Pretrained models + adapters)

*Goal: Add functional prediction and motifâ€‘conditioned generation while keeping the same 6â€‘step lens.*

**Projects**

1. **Protein Classifier with LoRA** (ESMâ€‘2 35M / ProtT5 60M).

* Fineâ€‘tune LoRA on labelled data (EC classes, AMR genes).
* Evaluate AUROC/F1; run Steps 2, 5, 6 on the adapterâ€™s embedding space.

2. **Functionâ€‘Conditioned Generator**

* Add control tokens (e.g., `<EC_3>`).
* Validate motif preservation in outputs; run Step 4 on controlled prefixes.

3. **Motif Coâ€‘occurrence Graphs**

* Build motif graphs from hiddenâ€‘state clusters; relate to function labels; inspect with Steps 2â€“3.

**Hardware**: 16 GB MacBook with quantization+LoRA.
**Outcome**: Strong lightweight classifiers, interpretable embeddings, motifâ€‘aware generation.

---

## Stage 3 â€“ Stretch Goals (Cloud or Cluster)

*Goal: Approach genuine design tasks with the interpretability pipeline as a safety net.*

**Projects**

1. **Largeâ€‘Backbone Fineâ€‘Tuning** (150Mâ€“650M ESM/ProGen).

* Keep Steps 1â€“6 to diagnose regressions and capacityâ€‘driven gains.

2. **Motifâ€‘Guided Design**

* Compose motifs A+B with learned linkers; validate with AF/ColabFold; iterate.

3. **Toward Deâ€‘novo Design**

* Combine LM generation with structure/energy scores; use RL or preference optimization.
* Use Steps 2â€“6 to ensure interpretability and avoid modeâ€‘collapse.

**Hardware**: Cloud GPUs (A100/H100) or university cluster.
**Outcome**: First deâ€‘novo functional design experiments.

---

## ğŸš€ Suggested Path (Learning Ladder)

1. **Start Small** â€” train minimal codon LM; run Steps **1â€“6**.
2. **Probe** â€” mutation heatmaps + motif clustering; relate to Steps **2,5**.
3. **Scale Up** â€” widen/deepen; compare runs with `compare_runs.py` (see below).
4. **Upgrade** â€” adapterâ€‘tune ESMâ€‘2 35M; interpret with Steps **2â€“6**.
5. **Stretch** â€” move to cloud; motifâ€‘guided design with continual interpretability checks.

---

## ğŸ“Š Comparing Runs ("growth" in language understanding)

Script: `compare_runs.py`
Inputs per run: `metrics` (val perplexity), `tables/embed_quality.txt` (silhouette, PCA var), `tables/probe_results.csv` (AA/polarity/hydropathy/start/stop).
Output: `runs/_summary/summary.csv` with columns `[run_id, val_ppl, silhouette, probe_aa, probe_class_pol, probe_class_hydro, probe_is_stop, probe_is_start, â€¦]`.

**What should improve with capacity/epochs?**

* â†“ Validation perplexity.
* â†‘ Embedding silhouette by AA identity.
* â†‘ Linearâ€‘probe accuracies (AA, polarity, hydropathy).
* Crisper attention specializations; stronger context sensitivity in Step 4; clearer saliency structure in Step 5.

---

## âœ… Summary

On laptops you can **train toy codon LMs and run a principled 6â€‘step interpretability suite** after every run. The same pipeline scales to adapters and large models, giving you a consistent, biologyâ€‘aware view of how the modelâ€™s â€œlanguageâ€ grows with capacity.
4. **Remote Bioinformatics Integrations (optional)**

* Add optional remote BLAST/EBI requests with caching and rateâ€‘limits to annotate generated sequences; keep disabled by default (localâ€‘first philosophy).
* Roadmap item only; not implemented yet.
