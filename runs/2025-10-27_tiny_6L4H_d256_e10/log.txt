[info] run_id=2025-10-27_tiny_6L4H_d256_e10
[info] config=configs/tiny_mps.yaml
[hardware] date: 2025-10-27 17:59:33 UTC
[hardware] uname: Darwin Mac 25.0.0 Darwin Kernel Version 25.0.0: Wed Sep 17 21:38:03 PDT 2025; root:xnu-12377.1.9~141/RELEASE_ARM64_T8112 arm64
[hardware] cpu: Apple M2
[hardware] python: 3.11.13
[hardware] torch: 2.2.2
[hardware] mps_available: True
[hardware] cuda_available: False
[hardware] cuda_device_count: 0
[config] snapshot:
[config] vocab_size: 69
[config] block_size: 256
[config] n_layer: 6
[config] n_head: 4
[config] n_embd: 256 
[config] dropout: 0.1
[config] 
[config] batch_size: 2
[config] grad_accum_steps: 16
[config] lr: 0.0003
[config] weight_decay: 0.05
[config] epochs: 10
[config] 
[config] optimizer: "adamw"
[config] amp: true
[config] use_checkpoint: true 
[config] scheduler: "plateau"
[config] label_smoothing: 0.05
[config] warmup_steps: 300   # cosine or plateau
[config] min_lr: 1e-5
[config] plateau_patience: 2
[config] early_stop_patience: 5
[config] 
[config] out_dir: outputs/checkpoints
[config] scores_dir: outputs/scores
[config] log_csv: "curves.csv"
[config] seed: 1337
[config] itos_path: data/processed/itos_codon.txt
[config] saliency_window: 9
[config] saliency_top: 20
[extract] wrote 4236 CDS with meta.
[tokenize] wrote 4236 sequences → data/processed/codon_ids.txt | vocab size 69 | itos data/processed/itos_codon.txt
[build] train: (6864, 256) → data/processed/train_bs256.npz
[build] val: (762, 256) → data/processed/val_bs256.npz
[build] test: (846, 256) → data/processed/test_bs256.npz
[run] id=2025-10-27_tiny_6L4H_d256_e10
[paths] ckpts=outputs/checkpoints/2025-10-27_tiny_6L4H_d256_e10 scores=outputs/scores/2025-10-27_tiny_6L4H_d256_e10 log_csv=/Users/User/github/genomics-lm/outputs/scores/2025-10-27_tiny_6L4H_d256_e10/curves.csv
[epoch 1] train 50.200 | val 5.842 | ppl 344.40 | lr 2.14e-04
[epoch 2] train 5.827 | val 4.123 | ppl 61.75 | lr 3.00e-04
[epoch 3] train 5.590 | val 4.086 | ppl 59.52 | lr 3.00e-04
[epoch 4] train 5.593 | val 4.059 | ppl 57.93 | lr 3.00e-04
[epoch 5] train 5.532 | val 4.034 | ppl 56.48 | lr 3.00e-04
[epoch 6] train 5.498 | val 4.020 | ppl 55.72 | lr 3.00e-04
[epoch 7] train 5.454 | val 4.021 | ppl 55.74 | lr 3.00e-04
[epoch 8] train 5.396 | val 4.017 | ppl 55.56 | lr 3.00e-04
[epoch 9] train 5.330 | val 4.014 | ppl 55.40 | lr 3.00e-04
[epoch 10] train 5.259 | val 4.015 | ppl 55.42 | lr 3.00e-04
[state] missing: []
[state] unexpected: []
[state] missing: []
[state] unexpected: []
[state] missing: []
[state] unexpected: []
[save] outputs/scores/2025-10-27_tiny_6L4H_d256_e10/one_cds__best.tsv

[state] missing: []
[state] unexpected: []
Using TinyGPT from: /Users/User/github/genomics-lm/src/codonlm/model_tiny_gpt.py
Model cfg: {'vocab_size': 69, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.1, 'batch_size': 2, 'grad_accum_steps': 16, 'lr': 0.0003, 'weight_decay': 0.05, 'epochs': 10, 'optimizer': 'adamw', 'amp': True, 'use_checkpoint': True, 'scheduler': 'plateau', 'label_smoothing': 0.05, 'warmup_steps': 300, 'min_lr': '1e-5', 'plateau_patience': 2, 'early_stop_patience': 5, 'out_dir': 'outputs/checkpoints', 'scores_dir': 'outputs/scores', 'log_csv': 'curves.csv', 'seed': 1337, 'itos_path': 'data/processed/itos_codon.txt', 'saliency_window': 9, 'saliency_top': 20, 'run_id': '2025-10-27_tiny_6L4H_d256_e10'}
Model device: mps:0
MPS available: True
[motifs] collected windows: 1702272
[motifs] inertia: 18.420873641967773
[motifs] saved outputs/motif_clusters.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/User/github/genomics-lm/scripts/collect_artifacts_yaml.py", line 28, in <module>
    from ._shared import (
  File "/Users/User/github/genomics-lm/scripts/_shared.py", line 28, in <module>
    @dataclass
     ^^^^^^^^^
  File "/Users/User/anaconda3/envs/codonlm/lib/python3.11/dataclasses.py", line 1232, in dataclass
    return wrap(cls)
           ^^^^^^^^^
  File "/Users/User/anaconda3/envs/codonlm/lib/python3.11/dataclasses.py", line 1222, in wrap
    return _process_class(cls, init, repr, eq, order, unsafe_hash,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/User/anaconda3/envs/codonlm/lib/python3.11/dataclasses.py", line 1027, in _process_class
    _init_fn(all_init_fields,
  File "/Users/User/anaconda3/envs/codonlm/lib/python3.11/dataclasses.py", line 545, in _init_fn
    raise TypeError(f'non-default argument {f.name!r} '
TypeError: non-default argument 'vocab_size' follows default argument
[timing] training_sec=989
[timing] total_sec=1324
